{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quizlet Flashcard Preprocessing\n",
    "\n",
    "This notebook loads Quizlet flashcard exports from the `flashcards/` directory, chunks them, embeds them, and stores them in a vector database.\n",
    "\n",
    "**Supported Formats:**\n",
    "- JSON files: `[{\"term\": \"...\", \"definition\": \"...\"}, ...]`\n",
    "- Tab-separated text files (classic Quizlet export)\n",
    "- CSV files (comma-separated)\n",
    "\n",
    "**Features:**\n",
    "- Automatically scans `flashcards/` directory for .json, .txt, and .csv files\n",
    "- Auto-detects file format\n",
    "- Tracks which files have been processed (avoids duplicates)\n",
    "- Processes only new/modified files\n",
    "- Creates/updates vector store incrementally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alan\\anaconda3\\envs\\cs4100-project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from quizlet_rag import QuizletRAGPipeline\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "FLASHCARDS_DIR = Path(\"flashcards\")\n",
    "VECTOR_DB_PATH = \"./quizlet_db\"\n",
    "COLLECTION_NAME = \"quizlet_flashcards\"\n",
    "TRACKING_FILE = \"processed_files.json\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Supported file extensions\n",
    "FILE_EXTENSIONS = [\"*.json\", \"*.txt\", \"*.tsv\", \"*.csv\"]\n",
    "\n",
    "# Default delimiter for text files (can be overridden per file)\n",
    "DEFAULT_DELIMITER = \"\\t\"\n",
    "\n",
    "# Chunking strategy\n",
    "CHUNK_STRATEGY = \"no_split\" # Options: 'no_split', 'by_term', 'recursive'\n",
    "\n",
    "# Create flashcards directory if it doesn't exist\n",
    "FLASHCARDS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_files():\n",
    "    \"\"\"Load the list of already processed files.\"\"\"\n",
    "    if Path(TRACKING_FILE).exists():\n",
    "        with open(TRACKING_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_processed_files(processed_files):\n",
    "    \"\"\"Save the list of processed files.\"\"\"\n",
    "    with open(TRACKING_FILE, 'w') as f:\n",
    "        json.dump(processed_files, indent=2, fp=f)\n",
    "\n",
    "def get_new_files(flashcards_dir, processed_files):\n",
    "    \"\"\"Get list of new files that haven't been processed.\"\"\"\n",
    "    all_files = []\n",
    "    \n",
    "    # Scan for all supported file types\n",
    "    for pattern in FILE_EXTENSIONS:\n",
    "        all_files.extend(flashcards_dir.glob(pattern))\n",
    "    \n",
    "    new_files = []\n",
    "    \n",
    "    for file_path in all_files:\n",
    "        file_key = str(file_path.name)\n",
    "        file_modified = file_path.stat().st_mtime\n",
    "        \n",
    "        # Check if file is new or has been modified\n",
    "        if file_key not in processed_files or processed_files[file_key][\"modified_time\"] < file_modified:\n",
    "            new_files.append(file_path)\n",
    "    \n",
    "    return new_files\n",
    "\n",
    "def mark_files_as_processed(file_paths, processed_files):\n",
    "    \"\"\"Mark files as processed with timestamp and format info.\"\"\"\n",
    "    for file_path in file_paths:\n",
    "        file_key = str(file_path.name)\n",
    "        file_format = \"json\" if file_path.suffix.lower() == \".json\" else \"text\"\n",
    "        \n",
    "        processed_files[file_key] = {\n",
    "            \"processed_at\": datetime.now().isoformat(),\n",
    "            \"modified_time\": file_path.stat().st_mtime,\n",
    "            \"path\": str(file_path),\n",
    "            \"format\": file_format,\n",
    "            \"extension\": file_path.suffix\n",
    "        }\n",
    "    return processed_files\n",
    "\n",
    "def detect_delimiter(file_path):\n",
    "    \"\"\"Detect delimiter for text files.\"\"\"\n",
    "    if file_path.suffix.lower() == \".csv\":\n",
    "        return \",\"\n",
    "    elif file_path.suffix.lower() == \".tsv\":\n",
    "        return \"\\t\"\n",
    "    else:\n",
    "        # Default to tab for .txt files\n",
    "        return \"\\t\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RAG pipeline\n",
    "pipeline = QuizletRAGPipeline(\n",
    "    embedding_model=EMBEDDING_MODEL,\n",
    "    vector_store_path=VECTOR_DB_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scan for New Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to process:\n",
      "  1. ai-1.json (JSON, 20.8 KB)\n",
      "  2. ai-2.json (JSON, 7.0 KB)\n",
      "  3. ai-3.json (JSON, 10.3 KB)\n",
      "  4. ai-4.json (JSON, 16.4 KB)\n",
      "  5. ai-5.json (JSON, 3.8 KB)\n"
     ]
    }
   ],
   "source": [
    "# Load tracking information\n",
    "processed_files = load_processed_files()\n",
    "# Find new files\n",
    "new_files = get_new_files(FLASHCARDS_DIR, processed_files)\n",
    "\n",
    "if new_files:\n",
    "    print(\"Files to process:\")\n",
    "    for i, file_path in enumerate(new_files, 1):\n",
    "        file_format = \"JSON\" if file_path.suffix.lower() == \".json\" else \"TEXT\"\n",
    "        file_size = file_path.stat().st_size / 1024 # Size in KB\n",
    "        print(f\"  {i}. {file_path.name} ({file_format}, {file_size:.1f} KB)\")\n",
    "else:\n",
    "    print(\"No new files to process. All flashcards are up to date!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process New Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Loading flashcards...\n",
      "   Format detection: AUTO\n",
      "   • ai-1.json: JSON format\n",
      "   • ai-2.json: JSON format\n",
      "   • ai-3.json: JSON format\n",
      "   • ai-4.json: JSON format\n",
      "   • ai-5.json: JSON format\n",
      "✓ Loaded 277 documents from 5 file(s)\n",
      "\n",
      "2. Chunking documents...\n",
      "✓ Using 277 flashcards as-is (no splitting)\n",
      "\n",
      "3. Embedding and storing in vector database...\n",
      "   Creating new vector store...\n",
      "⏳ Creating embeddings for 277 documents...\n",
      "✓ Vector store created at ./quizlet_db\n",
      "\n",
      "4. Updating tracking file...\n",
      "\n",
      "Total files processed: 5\n",
      "Documents added this run: 277\n",
      "Vector store location: ./quizlet_db\n"
     ]
    }
   ],
   "source": [
    "if new_files:\n",
    "    # Convert Path objects to strings\n",
    "    file_paths_str = [str(f) for f in new_files]\n",
    "    \n",
    "    # Load flashcards (auto-detects format)\n",
    "    print(\"\\n1. Loading flashcards...\")\n",
    "    print(\"   Format detection: AUTO\")\n",
    "    \n",
    "    # Show what format each file is detected as\n",
    "    for file_path in new_files:\n",
    "        file_format = \"JSON\" if file_path.suffix.lower() == \".json\" else \"TEXT\"\n",
    "        delimiter = detect_delimiter(file_path) if file_format == \"TEXT\" else \"N/A\"\n",
    "        delimiter_name = \"tab\" if delimiter == \"\\t\" else \"comma\" if delimiter == \",\" else delimiter\n",
    "        print(f\"   • {file_path.name}: {file_format} format\" + \n",
    "              (f\" ({delimiter_name}-separated)\" if file_format == \"TEXT\" else \"\"))\n",
    "    \n",
    "    docs = pipeline.load_flashcards(\n",
    "        file_paths=file_paths_str,\n",
    "        delimiter=DEFAULT_DELIMITER, # Will be auto-detected per file\n",
    "        chunk_strategy=\"individual\"\n",
    "    )\n",
    "    \n",
    "    # Chunk documents\n",
    "    print(\"\\n2. Chunking documents...\")\n",
    "    chunks = pipeline.chunk_documents(docs, strategy=CHUNK_STRATEGY)\n",
    "    \n",
    "    # Create or update vector store\n",
    "    print(\"\\n3. Embedding and storing in vector database...\")\n",
    "    \n",
    "    # Check if vector store already exists\n",
    "    vectorstore_exists = Path(VECTOR_DB_PATH).exists()\n",
    "    \n",
    "    if vectorstore_exists:\n",
    "        print(\"   Loading existing vector store...\")\n",
    "        pipeline.load_existing_vectorstore(collection_name=COLLECTION_NAME)\n",
    "        print(\"   Adding new documents...\")\n",
    "        pipeline.vectorstore.add_documents(chunks)\n",
    "        print(f\"   Added {len(chunks)} chunks to existing vector store\")\n",
    "    else:\n",
    "        print(\"   Creating new vector store...\")\n",
    "        pipeline.create_vectorstore(\n",
    "            documents=chunks,\n",
    "            collection_name=COLLECTION_NAME\n",
    "        )\n",
    "    \n",
    "    # Mark files as processed\n",
    "    print(\"\\n4. Updating tracking file...\")\n",
    "    processed_files = mark_files_as_processed(new_files, processed_files)\n",
    "    save_processed_files(processed_files)\n",
    "\n",
    "    print(f\"\\nTotal files processed: {len(processed_files)}\")\n",
    "    print(f\"Documents added this run: {len(chunks)}\")\n",
    "    print(f\"Vector store location: {VECTOR_DB_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Processing Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Files Summary:\n",
      "\n",
      "JSON Files (5):\n",
      "  • ai-1.json\n",
      "    Processed: 2025-11-04T12:35:40\n",
      "  • ai-2.json\n",
      "    Processed: 2025-11-04T12:35:40\n",
      "  • ai-3.json\n",
      "    Processed: 2025-11-04T12:35:40\n",
      "  • ai-4.json\n",
      "    Processed: 2025-11-04T12:35:40\n",
      "  • ai-5.json\n",
      "    Processed: 2025-11-04T12:35:40\n",
      "\n",
      "Total: 5 file(s) processed\n"
     ]
    }
   ],
   "source": [
    "# Display summary of all processed files\n",
    "print(\"Processed Files Summary:\")\n",
    "\n",
    "processed_files = load_processed_files()\n",
    "if processed_files:\n",
    "    # Group by format\n",
    "    json_files = []\n",
    "    text_files = []\n",
    "    \n",
    "    for filename, info in processed_files.items():\n",
    "        if info.get(\"format\") == \"json\":\n",
    "            json_files.append((filename, info))\n",
    "        else:\n",
    "            text_files.append((filename, info))\n",
    "    \n",
    "    if json_files:\n",
    "        print(f\"\\nJSON Files ({len(json_files)}):\")\n",
    "        for filename, info in sorted(json_files):\n",
    "            print(f\"  • {filename}\")\n",
    "            print(f\"    Processed: {info['processed_at'][:19]}\")\n",
    "    \n",
    "    if text_files:\n",
    "        print(f\"\\nText Files ({len(text_files)}):\")\n",
    "        for filename, info in sorted(text_files):\n",
    "            ext = info.get('extension', 'unknown')\n",
    "            print(f\"  • {filename} ({ext})\")\n",
    "            print(f\"    Processed: {info['processed_at'][:19]}\")\n",
    "    \n",
    "    print(f\"\\nTotal: {len(processed_files)} file(s) processed\")\n",
    "else:\n",
    "    print(\"No files have been processed yet.\")\n",
    "    print(\"\\nTo get started:\")\n",
    "    print(\"1. Export your Quizlet flashcard sets in one of these formats:\")\n",
    "    print(\"   • JSON: [{\\\"term\\\": \\\"...\\\", \\\"definition\\\": \\\"...\\\"}]\")\n",
    "    print(\"   • Tab-separated text (classic Quizlet export)\")\n",
    "    print(\"   • CSV (comma-separated)\")\n",
    "    print(f\"2. Place the files in the '{FLASHCARDS_DIR}' directory\")\n",
    "    print(\"3. Run this notebook again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset/Clear Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to reset tracking file (forces reprocessing of all files)\n",
    "# import os\n",
    "# if Path(TRACKING_FILE).exists():\n",
    "#     os.remove(TRACKING_FILE)\n",
    "#     print(f\"✓ Deleted {TRACKING_FILE}\")\n",
    "\n",
    "# Uncomment to delete vector store (complete reset)\n",
    "# import shutil\n",
    "# if Path(VECTOR_DB_PATH).exists():\n",
    "#     shutil.rmtree(VECTOR_DB_PATH)\n",
    "#     print(f\"✓ Deleted vector store at {VECTOR_DB_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4100-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
